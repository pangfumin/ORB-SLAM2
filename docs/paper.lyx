#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans helvet
\font_typewriter courier
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 23.5pt
\topmargin 1in
\rightmargin 23.5pt
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Monocular Vision-based Localization using ORB-SLAM with LIDAR-aided Mapping
 in Tsukuba Challenge
\end_layout

\begin_layout Author
Adi Sujiwo, Tomohito Ando, Eijiro Takeuchi, and Yoshiki Ninomiya
\end_layout

\begin_layout Abstract
In 2015 Tsukuba Challenge, we have realized an implementation of vision-based
 localization based on ORB-SLAM.
 Our method combined mapping based on ORB-SLAM and Velodyne LIDAR SLAM,
 and utilized these maps in localization process using solely monocular
 camera.
 The combined maps delivered better accuracy compared to original ORB-SLAM,
 which suffers from scale ambiguities and experiences map distance distortion.
 This paper reports our experience in using ORB-SLAM for visual localization,
 along with difficulties that we encounter.
\end_layout

\begin_layout Abstract
Keyword: Visual Localization, Autonomous Vehicle, Field Robotics, Tsukuba
 Challenge
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Background
\end_layout

\begin_layout Standard
The importance of vision-based localization can not be overstated.
 In applications like autonomous vehicle, this localization method will
 significantly reduce cost for manufacturer, which previously must use LIDAR-bas
ed solutions.
 However, vision-based methods are usually avoided due to inaccuracy when
 compared to LIDAR-based methods, in addition to lack of metrically correctness.
\end_layout

\begin_layout Standard
To the best of our knowledge, it is the first implementation of monocular
 vision-based localization in Tsukuba Challenge.
 Previous methods for localization usually employs LIDAR-based methods,
 either 2D or 3D.
\end_layout

\begin_layout Standard
Our primary reason for choosing ORB-SLAM is related in our work in autonomous
 vehicles, which requires localization methods using monocular camera and
 provides capability for lifelong mapping.
 
\end_layout

\begin_layout Standard
Our challenge ...
\end_layout

\begin_layout Subsection
Objectives
\end_layout

\begin_layout Standard
In the Tsukuba Challenge, we would like to evaluate effectiveness of ORB-SLAM
 and compare the localization results against LIDAR-based method as ground
 truth.
\end_layout

\begin_layout Section
Related Works
\end_layout

\begin_layout Subsection
Monocular SLAM
\end_layout

\begin_layout Standard
Besides ORB-SLAM, there have been a number of monocular SLAM with complete
 public implementation.
 Of particular mention are PTAM by 
\begin_inset CommandInset citation
LatexCommand cite
key "klein2007parallel"

\end_inset

, and LSD-SLAM by 
\begin_inset CommandInset citation
LatexCommand cite
key "engel2014lsdslam"

\end_inset

.
\end_layout

\begin_layout Standard
ORB-SLAM itself was described in 
\begin_inset CommandInset citation
LatexCommand cite
key "mur-artal2015orbslam"

\end_inset

.
 Of particular note, the ORB-SLAM uses ORB (Oriented FAST, Rotated BRIEF)
 as main feature detector, and bag-of-words method as place recognition.
 Contrary to previous methods, ORB-SLAM generates relatively sparse maps
 due to us of extracted feature points rather than whole scenes.
 This gives potentially smaller maps and faster processing time.
\end_layout

\begin_layout Standard
All monocular SLAM methods are based on 3D reconstruction from multiple
 view of scenes 
\begin_inset CommandInset citation
LatexCommand cite
key "fuentes-pacheco2015visualsimultaneous"

\end_inset

, which in turn are based on Structure from Motion.
 As stated in 
\begin_inset CommandInset citation
LatexCommand cite
key "szeliski1997shapeambiguities"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "lourakis2013accurate"

\end_inset

, all monocular structure from motion methods inherit common scale ambiguities,
 which consists in the fact that the recovered 3D structures and camera
 motion are defined up to an unknown scale factor which cannot be determined
 from image streams alone.
 This is because if the scene and camera are scaled together, this change
 will not be indistinguishable in the captured images.
\end_layout

\begin_layout Standard
Another effort for LIDAR-assisted mapping with visual localization is works
 of Wolcott in 
\begin_inset CommandInset citation
LatexCommand cite
key "wolcott2014visuallocalization"

\end_inset

.
 Here, frames from camera is directly compared with synthetic images rendered
 from LIDAR-based maps, thus creating indirect image-to-image matches using
 Mutual Information as basis for scoring.
\end_layout

\begin_layout Subsection
Velodyne SLAM
\end_layout

\begin_layout Standard
Velodyne-based SLAM is explained in 
\begin_inset CommandInset citation
LatexCommand cite
key "moosmann2011velodyne"

\end_inset

.
 This SLAM method is popular for autonomous vehicle applications, and is
 capable to provide accurate maps andlocalization.
\end_layout

\begin_layout Standard
Velodyne-based SLAM takes 3D laser scans as input, and compose the map using
 any popular scan matching algorithms from those scans (for example, normal
 distribution transform 
\begin_inset CommandInset citation
LatexCommand cite
key "biber2003thenormal"

\end_inset

).
 
\end_layout

\begin_layout Section
Monocular Vision-Based Localization
\end_layout

\begin_layout Standard
In order to provide multiple time localization, our implementation of ORB-SLAM
 consists of two parts: mapping and localization-only.
 The mapping process runs similar with original implementation, with addition
 of map storage in the end of mapping run.
 Meanwhile, the localization-only process starts with map restoration from
 data saved previously by mapping process.
 Next, localization-only proceeds similar with mapping.
 However, map modification is disabled in relocalization process.
\end_layout

\begin_layout Subsection
Description of ORB-SLAM
\end_layout

\begin_layout Standard
The ORB-SLAM main process creates an environmental map which consists of
 keyframes and map points.
 Each keyframe stores its position in ORB-SLAM coordinate and a list of
 2D feature points.
 These feature points are computed from ORB feature points in tracking subproces
s.
 
\end_layout

\begin_layout Standard
The whole ORB-SLAM process consists of three parallel threads: tracking,
 local mapping and loop closing.
 The tracking is responsible to localize the camera with every frame, and
 decides when to insert a new keyframe.
 
\end_layout

\begin_layout Subsubsection
Feature Detection
\end_layout

\begin_layout Standard
The first step in all 3D reconstruction is to find feature points in each
 frame.
 ORB-SLAM uses ORB (Oriented FAST, Rotated BRIEF) described in 
\begin_inset CommandInset citation
LatexCommand cite
key "rublee2011orban"

\end_inset

.
 This detector has advantages such as faster computation and lower storage
 requirement (each descriptor only needs 32 bytes), in addition to resistance
 against rotation and noise.
 By default, number of extracted ORB points is fixed at 1000.
 However, we found that increasing this number up to 2500 gives many benefits,
 including faster map initialization and resistance against image flaws
 such as partial occlusion and lens flares.
\end_layout

\begin_layout Subsubsection
Map Initialization
\end_layout

\begin_layout Standard
The goal of the map initialization is to compute the relative pose between
 two frames to triangulate an initial set of map points
\begin_inset CommandInset citation
LatexCommand cite
key "mur-artal2015orbslam"

\end_inset

, which then will be used for keyframe tracking.
 ORB-SLAM uses combination of homography and fundamental matrices inside
 RANSAC scheme in order to build motion and structure recovery as described
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "faugeras1988motionand"

\end_inset

.
 When this stage is succesful, the system will have an initial set of keyframes
 and map points for which tracking may proceed.
 However, sometimes tracking fails shortly after initial map is built.
 When this is the case, initial map is deleted and initialization process
 will start over.
\end_layout

\begin_layout Subsubsection
Tracking and Local Mapping
\end_layout

\begin_layout Standard
The tracking thread is responsible for providing localization and map building.
 After ORB corners are detected, the tracking thread develops map incrementally
 over recovered 3D map points, meanwhile computing camera poses.
 To speed up the process, tracking works in a smaller subset of the whole
 map, called local map, that covers current visible and a number of connected
 keyframes.
 Tracking thread also performs map 
\begin_inset Quotes eld
\end_inset

clean-up
\begin_inset Quotes erd
\end_inset

, which is an act of culling bad map points and keyframes.
 
\end_layout

\begin_layout Standard
To perform tracking, there are three modes that can be used.
 First is relocalization by searching all keyframes by bag of words; this
 is the slowest but indispensable when recovering from lost tracking.
 Second is tracking the local map as described above.
 Alternatively, third choice is tracking using constant velocity model.
 The third choice is fastest and may be most often used mode.
 However, it may be inaccurate.
\end_layout

\begin_layout Standard
Occasionally, tracking thread will perform local bundle adjustment (BA)
 to optimize current local map.
 This action moves the keyframes in local map, and potentially marks some
 keyframes as outliers for removal.
\end_layout

\begin_layout Subsubsection
Loop Closing
\end_layout

\begin_layout Standard
Loop-closure detection is crucial for enhancing the accuracy of SLAM algorithms,
 both topological and metrical.
 This problem consists of detecting when the robot has returned to a past
 location after having discovered new terrain for a while.
 Such detection makes it possible to increase the precision of the actual
 pose estimation.
\end_layout

\begin_layout Standard
Essentially, loop closing in ORB-SLAM uses image-to-map approach 
\begin_inset CommandInset citation
LatexCommand cite
key "williams2009acomparison"

\end_inset

 for loop closing.
 First, it takes last processed keyframe and search the loop candidate keyframes
 in local map using bag of words 
\begin_inset CommandInset citation
LatexCommand cite
key "galvez-lopez2012bagsof"

\end_inset

.
 Next, similarity transformation is computed.
 Loop correction is performed by inserting new edges in covisibility graph
 and fixing connectivity between loop candidate and surrounding keyframes.
 Next, ORB-SLAM performs pose graph optimization that distributes loop closing
 errors by moving the candidate and its connected keyframes.
\end_layout

\begin_layout Subsection
Problems in ORB-SLAM
\end_layout

\begin_layout Subsubsection
Scale Problem
\end_layout

\begin_layout Standard
ORB-SLAM emits localization results in its own coordinate system, that are
 not free from distortion.
 This problem is inherent in almost all vision-based localization methods.
 In some occasion, result maps may suffer heavy deformation, as illustrated
 in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ORB-SLAM-robot-trajectory"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename scale problem.eps
	lyxscale 70
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
ORB-SLAM map trajectories from different time are plotted in (1) and (2).
 (3) is ground truth.
 (4) is zoomed part of red rectangle in (1).
\begin_inset CommandInset label
LatexCommand label
name "fig:ORB-SLAM-robot-trajectory"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:ORB-SLAM-robot-trajectory"

\end_inset

, it is clear that ORB-SLAM generates very deformed trajectory shapes compared
 to ground truth (trajectory generated by Velodyne SLAM at corresponding
 time).
 By zooming-in parts in red square, it is revealed that most later keyframes
 clump in a small area of the ORB-SLAM map.
 By comparing (1) and (4), clearly ...
\end_layout

\begin_layout Subsubsection
Lack of Support for Lifelong Mapping
\end_layout

\begin_layout Standard
Original design for ORB-SLAM is single run for both localization and mapping,
 and there is not any features to store in-memory map.
 However, it is desirable that mapping can be done multiple times with same
 path, so map saving and restoration is essential.
 This feature is also useful for improving map robustness in changing condition
 
\begin_inset CommandInset citation
LatexCommand cite
key "konolige2009towards"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Sudden Vision Occlusion
\end_layout

\begin_layout Standard
Any disturbances in camera vision that make it unable to view previously
 tracked feature points may cause ORB-SLAM to lost tracking.
 These include vision occlusion on behalf of camera and sudden rotation
 of the robot.
 The default behaviour of ORB-SLAM to reacquire tracking after lost event
 is by performing relocalization from last keyframe (i.e.
 guess pose from last known position).
 However, this prevent the system to reacquire position especially when
 the robot never reverses motion, and contrary to described behaviour in
 original paper of ORB-SLAM.
 Our solution is to force relocalization by searching all keyframes in database.
\end_layout

\begin_layout Section
ORB-SLAM with LIDAR-aided Mapping
\end_layout

\begin_layout Subsection
Scale Correction in Localization
\end_layout

\begin_layout Standard
State our motivation for using velodyne
\end_layout

\begin_layout Standard
Scale problem can be solved if distance of two particular points in ORB-SLAM
 coordinate is known based on external reference.
 In longer run, this association must be done correctly so that error accumulati
on of scale correction is eliminated.
 Therefore, the external reference must have high accuracy.
 In our case, we chose Velodyne-based SLAM as reference due to immediate
 availability.
 Other positioning method may also be used, such as GPS or odometry as long
 as error corrections are provided 
\begin_inset CommandInset citation
LatexCommand cite
key "burschka2004vgpsslam"

\end_inset

.
\end_layout

\begin_layout Standard
In the localization-only process, the system depends solely on ORB-SLAM
 method.
 Therefore, external methods such as LIDAR SLAM are not required.
 However, the system modifies position value to correct distance deformation
 according to this formula.
 Each pose variable represents translation and rotation: 
\begin_inset Formula $P=(\mathbf{t},\mathbf{q})=(x,y,z,q_{x},q_{y},q_{z},q_{w})$
\end_inset


\end_layout

\begin_layout Enumerate
Take original ORB-SLAM position as 
\begin_inset Formula $P_{o}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Search the nearest keyframe in map from 
\begin_inset Formula $P_{o}$
\end_inset

 using previous octree as 
\begin_inset Formula $P_{k}$
\end_inset

.
 From here, we take the corresponding Velodyne SLAM position 
\begin_inset Formula $P_{n}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Find the previous offset keyframe 
\begin_inset Formula $P_{k'}$
\end_inset

 and its corresponding Velodyne SLAM position 
\begin_inset Formula $P_{n'}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Compute scale correction factor: 
\begin_inset Formula 
\[
s=\frac{\left\Vert \mathbf{t}_{n'}-\mathbf{t}_{n}\right\Vert }{\left\Vert \mathbf{t}_{k'}-\mathbf{t}_{k}\right\Vert }
\]

\end_inset


\end_layout

\begin_layout Enumerate
Apply distance scale to correct current position 
\begin_inset Formula $P_{r}=P_{o'}^{-1}P_{o}$
\end_inset

 :
\begin_inset Formula 
\[
P_{r'}=(st_{r},q_{r})
\]

\end_inset


\begin_inset Formula 
\[
P_{c}=P_{n}P_{r'}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 3d view of orb map.png
	lyxscale 30
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
3D view of Tsukuba Challenge map generated by NDT Scan Matching from Velodyne
 Scans
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Map Storage and Restoration
\end_layout

\begin_layout Standard
Map storage consists of three main parts: keyframes, map points, and keyframe
 relationships.
 Each keyframe stores camera pose 
\begin_inset Formula $P_{o}$
\end_inset

 in ORB-SLAM coordinate, camera intrinsic parameters, all ORB feature points
 recorded at keyframe creation, and LIDAR pose 
\begin_inset Formula $P_{n}$
\end_inset

 in NDT coordinate recorded at keyframe creation.
 
\end_layout

\begin_layout Standard
During map restoration, the system reconstructs the following data structures:
\end_layout

\begin_layout Enumerate
List of keyframes and their relationship.
\end_layout

\begin_layout Enumerate
Map point list
\end_layout

\begin_layout Enumerate
Octree of keyframe position in ORB-SLAM coordinate.
 This tree will be used for fast searching of keyframes during localization
 using augmented positioning.
\end_layout

\begin_layout Standard
By default, ORB-SLAM will try to find position against last keyframe when
 it ever get lost.
 However, it is impractical after map restoration, because last keyframe
 is unknown.
 Instead, we modify ORB-SLAM to force it finding the most appropriate keyframe
 using bag-of-word method.
 Keyframe search is also applied when the system lost tracking, to ensure
 that the system always get the keyframe as basis for relocalization.
 The drawback is that keyframe search using bag-of-word is slower than tracking
 using last keyframe.
\end_layout

\begin_layout Subsection
Using Multiple Maps
\end_layout

\begin_layout Standard
During experiments, we found that maps from same place but different time
 will deliver different results.
 These differences are visible on figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Using-Map-(M1)"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Using-Map-(M2)"

\end_inset

.
 Therefore it is logical to combine results from two maps in order to: 1)
 alternately provide localization whenever one of the maps fails; 2) reduce
 errors from all maps.
\end_layout

\begin_layout Standard
Our approach in fusing results from multiple maps is by averaging position
 data from each map.
 This process is performed each time any map (that runs in their own tracking
 threads) emits position data.
 A map is marked as fail when it does not emit position after a determined
 time interval.
\end_layout

\begin_layout Section
Evaluation in Tsukuba Challenge Environment
\end_layout

\begin_layout Standard
We performed three runs of robot by tracing the trajectory mandated by Tsukuba
 Challenge committee.
 In each run, we recorded camera images and performed localization using
 Velodyne LIDAR using two different computers due to high computational
 requirement.
 Clocks of both computers were not synchronized.
 From these runs, we created two maps for localization process.
 Velodyne LIDAR results would be used as ground truth for comparison.
 To reduce computation, camera resolution was reduced to 
\begin_inset Formula $800\times600$
\end_inset

 before processing.
\end_layout

\begin_layout Standard
Two runs were chosen as material for mapping with consideration that those
 runs spanned longest runs without any vision occlusion.
 However, in both run path loop were not executed.
 Therefore, we were not able to evaluate loop closing capability of ORB-SLAM.
 Next, we performed localization test in two runs using both maps.
\end_layout

\begin_layout Standard
The map trajectory that we use is drawn in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ORB-SLAM-robot-trajectory"

\end_inset

 (1) and (2).
 Note that those maps are heavily deformed compared to ground truth in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Ground-truth"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ndt-map-3.eps
	lyxscale 70
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Ground truth from Velodyne SLAM.
 In both runs for mapping, real trajectories of robot were about the same.
 This map is metrically correct.
\begin_inset CommandInset label
LatexCommand label
name "fig:Ground-truth"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Experimental Settings
\end_layout

\begin_layout Standard
Our robot is derived from Segway RMP-200 platform, using Grasshopper3 camera
 and Velodyne HDL-32 LIDAR.
 The robot ran on the mandated Tsukuba Challenge with speed under 1 m/s.
 In the course of run, robot may often encounter dynamic obstacles such
 as human or bycicle, which necessitated action by operator to either stop
 or maneuver the robot motion.
\end_layout

\begin_layout Standard
Our track in Tsukuba Challenge was vastly different from original ORB-SLAM
 paper evaluation.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename robot view.jpg
	lyxscale 30
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Robot for evaluation
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename starting point.jpg
	lyxscale 50
	width 95col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Starting Point
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Map Saving and Restoration
\end_layout

\begin_layout Standard
In our experience, map saving and restoration does not affect ORB-SLAM performan
ce.
 In addition, the system gains useful capability: map building can now be
 done incrementally, using same location in different time.
 This is useful for example, to build lifelong map in different situation
 such as weather and day/night.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename partial map.png
	lyxscale 70
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Partial map being built after loading
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
First Position Fix
\end_layout

\begin_layout Standard
To get initial position fix, ORB-SLAM performs keyframe search based on
 appearances of feature points.
 Search may returns more than one candidates, on which they will be evaluated
 on reprojection error basis.
 Only one candidate is accepted, and it must have at least 15 map points
 that match with feature points in current frame.
 In the evaluation run however, the system was slow to get the fix due to
 insufficient matches.
 One possible correction to fasten initial fix is by increasing number of
 feature points from ORB computation.
 However, this approach heavily slows down the search process, and does
 not always correlate to faster fix.
\end_layout

\begin_layout Standard
Map initialization is also another problem.
 During experiment, we found that initialization will succeed (and not getting
 false initialization) whenever robot is moved, either rotation or translation.
\end_layout

\begin_layout Subsection
Tracking and Relocalization
\end_layout

\begin_layout Standard
During our experiment, we found that ORB-SLAM is resistant against occasional
 and partial vision occlusion.
 Partial occlusion includes lens flares and human covering background images.
 Total occlusion however, may cause the system to miss tracking and difficult
 to recover.
 This lost tracking explains existence of blank areas below (part of trajectory
 that has no bold parts).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename tracking 1.png
	lyxscale 60
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
ORB-SLAM performed tracking
\begin_inset CommandInset label
LatexCommand label
name "fig:tracking"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Common situation and tracking of ORB-SLAM is depicted in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:tracking"

\end_inset

.
 The figure illustrates a frame, taken in Oshimizu park area with background
 of far buildings and some trees in foreground.
 There were also some people in the scene.
 Most of the ORB points (and map points, shown in green dots) fell in the
 trees and ground, but very few of those that were in background.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:traversing-the-map"

\end_inset

 depicts visualization of the robot when it traveled along previously created
 map.
 The map was created from different day.
 Note the slightly out-of-track position of the robot.
 In this situation, tracking was maintained.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename robot traveled along the map.png
	lyxscale 60
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Robot traversed previously created map
\begin_inset CommandInset label
LatexCommand label
name "fig:traversing-the-map"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:lost-tracking"

\end_inset

 depicts a situation when the robot performed violent rotation and about
 to lost tracking.
 Note the absence of ORB feature points in the right portion of image frame.
 At right, the axis shows that robot is in right track, but robot is oriented
 to a place without map points.
 The blue axis represents front.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename about to lose tracking.png
	lyxscale 60
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
ORB-SLAM about to lost tracking
\begin_inset CommandInset label
LatexCommand label
name "fig:lost-tracking"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Using-Map-(M1)"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Using-Map-(M2)"

\end_inset

 show the results from using map 1 and 2.
 Next figure (
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:joint-map-result"

\end_inset

) shows result using two maps.
 The differences of using both maps are: 1) map 1 is quicker in obtaining
 initial fix but has larger 
\begin_inset Quotes eld
\end_inset

blank spot
\begin_inset Quotes erd
\end_inset

 due to lost tracking; 2) map 2 delivers more coverage due to less lost
 tracking.
 In all maps, initial position fix were obtained at substantial time after
 motion were commenced.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename orb-1.svg
	lyxscale 40
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Using Map (M1)
\begin_inset CommandInset label
LatexCommand label
name "fig:Using-Map-(M1)"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename orb-2.svg
	lyxscale 40
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Using Map (M2)
\begin_inset CommandInset label
LatexCommand label
name "fig:Using-Map-(M2)"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename jointmap_result.svg
	lyxscale 90
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Localization Using Combined Maps (M1) and (M2)
\begin_inset CommandInset label
LatexCommand label
name "fig:joint-map-result"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Loop Closing
\end_layout

\begin_layout Standard
The Tsukuba Challenge track features some areas that are ideal to test loop
 closing subsystem of any SLAM system.
 However, ORB-SLAM was not able to perform loop closing when they were supposed
 to take place.
 One important area is the Oshimizu park subtrack, that features a small
 roundabout with far buildings as background.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename loop closing failure.png
	lyxscale 70
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
A particular place where loop closing failed to take place.
 Image contrast has been adjusted to lighten dark areas.
 Notice that most ORB points (green) fell in the background cloud.
\begin_inset CommandInset label
LatexCommand label
name "fig:loop-closing-failure"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Accuracy of Corrected Localization
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename path inspection.svg
	lyxscale 70
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Visual comparison of Modified ORB-SLAM trajectory against ground truth (blue
 curve).
 Map 1 result is depicted in green, while map 2 in red.
\begin_inset CommandInset label
LatexCommand label
name "fig:turning-map"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:turning-map"

\end_inset

 depicts a turning situation continued by straight path.
 In the turning, modified ORB-SLAM shows deviations compared to ground truth,
 while straight path has less deviation.
 It is also clear that different maps give different results, despite running
 in the exact same path and time.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename errors1.eps
	lyxscale 50
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename errors2.eps
	lyxscale 50
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename errorsj.eps
	lyxscale 50
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Error graphs from all maps used in the experiment
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="right" valignment="top" width="0">
<column alignment="right" valignment="top" width="0">
<column alignment="right" valignment="top" width="0">
<row>
<cell multirow="3" alignment="left" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Map
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Errors (in m)
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Average
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Std.
 Dev
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Maximum
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Map 1
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.08
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.21
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Map 2
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.06
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.09
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.67
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Joint Map
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.07
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.61
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Errors from modified ORB-SLAM compared to ground truth
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusions and Future Work
\end_layout

\begin_layout Standard
This work reported our approach for localization solutions in Tsukuba Challenge.
 Within limitations of our system, the experiments confirmed that vision-based
 localization using augmented maps from vision and LIDAR-based methods are
 capable to provide localization that are precise enough.
\end_layout

\begin_layout Standard
Contrary to original results, ORB-SLAM was unable to give excellent results
 in dynamic environment such as Tsukuba Challenge.
 We suspect that due to significant time between mapping and localization,
 ORB-SLAM was unable to perform place recognition using its bag-of-words
 method.
 This is supported from observation that most ORB feature points fell in
 the ground covered in falling leaves.
\end_layout

\begin_layout Standard
Despite a good results, there are still some room for improvements that
 we can propose.
 Primarily, there must be an effort to cover the whole spectrum of localization,
 critically when the vision-based localization fails.
 In this regard, we suggest next works to integrate other low-cost methods
 such as odometry and gyroscope solutions.
 In the mapping, we also suggest replacing the LIDAR to other global SLAM
 methods, such as GPS .
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "ZoteroLibrary"
options "ieeetr"

\end_inset


\end_layout

\end_body
\end_document
