#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Vision-based Localization and Navigation using ORB-SLAM in Tsukuba Challenge
\end_layout

\begin_layout Author
Adi Sujiwo and Eijiro Takeuchi
\end_layout

\begin_layout Abstract
In 2015 Tsukuba Challenge, we have realized an implementation of vision-based
 localization based on ORB-SLAM.
 Our method combined mapping based on ORB-SLAM and Velodyne LIDAR SLAM,
 and utilized these maps in localization process using solely monocular
 camera.
 This approach delivered better accuracy compared to original ORB-SLAM,
 which suffers from scale problems and experienced map distance distortion.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Background
\end_layout

\begin_layout Standard
The importance of vision-based localization can not be overstated.
 In applications like autonomous vehicle, this localization method will
 significantly reduce cost for manufacturer, which previously must use LIDAR-bas
ed solutions.
 However, vision-based methods are usually avoided due to inaccuracy when
 compared to LIDAR-based methods, in addition to lack of metrically correctness.
\end_layout

\begin_layout Standard
To the best of our knowledge, it is the first implementation of monocular
 vision-based localization in Tsukuba Challenge.
 Previous methods for localization usually employs LIDAR-based methods,
 either 2D or 3D.
\end_layout

\begin_layout Standard
Our primary reason for choosing ORB-SLAM is related in our work in autonomous
 vehicles, which requires localization methods using monocular camera and
 provides capability for lifelong mapping.
 
\end_layout

\begin_layout Subsection
Objectives
\end_layout

\begin_layout Standard
In the Tsukuba Challenge, we would like to evaluate effectiveness of ORB-SLAM
 and compare the localization results against LIDAR-based method as ground
 truth.
\end_layout

\begin_layout Section
Related Works
\end_layout

\begin_layout Subsection
Monocular SLAM
\end_layout

\begin_layout Standard
Besides ORB-SLAM, there have been a number of monocular SLAM with complete
 public implementation.
 Of particular mention are PTAM by 
\begin_inset CommandInset citation
LatexCommand cite
key "klein2007parallel"

\end_inset

, and LSD-SLAM by 
\begin_inset CommandInset citation
LatexCommand cite
key "engel2014lsdslam"

\end_inset

.
\end_layout

\begin_layout Standard
ORB-SLAM itself was described in 
\begin_inset CommandInset citation
LatexCommand cite
key "mur-artal2015orbslam"

\end_inset

.
\end_layout

\begin_layout Subsection
Velodyne SLAM
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "moosmann2011velodyne"

\end_inset

.
\end_layout

\begin_layout Section
Strategies for Vision-based Localization
\end_layout

\begin_layout Standard
In order to provide multiple time localization, our implementation of ORB-SLAM
 consists of two parts: mapping and localization-only.
 The mapping process runs similar with original implementation, with addition
 of map storage in the end of mapping run.
 Meanwhile, the localization-only process starts with map restoration from
 data saved previously by mapping process.
 Next, localization-only proceeds similar with mapping.
 However, map modification is disabled in relocalization process.
\end_layout

\begin_layout Subsection
Description of ORB-SLAM
\end_layout

\begin_layout Standard
The ORB-SLAM main process creates an environmental map which consists of
 keyframes and map points.
 Each keyframe stores its position in ORB-SLAM coordinate and a list of
 2D feature points.
 These feature points are computed from ORB feature points in tracking subproces
s.
 
\end_layout

\begin_layout Standard
The whole ORB-SLAM process consists of three parallel threads: tracking,
 local mapping and loop closing.
 The tracking is responsible to localize the camera with every frame, and
 decides when to insert a new keyframe.
 
\end_layout

\begin_layout Subsubsection
Feature Detection
\end_layout

\begin_layout Standard
The first step in all 3D reconstruction is to find feature points in each
 frame.
 ORB-SLAM uses ORB (Oriented FAST, Rotated BRIEF)
\end_layout

\begin_layout Subsubsection
Tracking
\end_layout

\begin_layout Subsubsection
Local Mapping
\end_layout

\begin_layout Subsubsection
Loop Closing
\end_layout

\begin_layout Subsection
Problems in Original Implementation of ORB-SLAM
\end_layout

\begin_layout Subsubsection
Scale Problem
\end_layout

\begin_layout Standard
ORB-SLAM emits localization results in its own coordinate system, which
 is not free from distortion.
 This problem is inherent in almost all vision-based localization methods.
 In some occasion, result maps may suffer heavy distance distortion, as
 illustrated below.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename scale problem.svg
	lyxscale 30
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
ORB-SLAM robot trajectory (top), ground truth (middle), and zoomed part
 of red square in top figure
\begin_inset CommandInset label
LatexCommand label
name "fig:ORB-SLAM-robot-trajectory"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:ORB-SLAM-robot-trajectory"

\end_inset

, it is clear that ORB-SLAM has very different trajectory shape compared
 to ground truth (trajectory generated by Velodyne SLAM at corresponding
 time).
 By zooming-in parts in red square, it is revealed that most later keyframes
 clump in a small area of the ORB-SLAM map.
\end_layout

\begin_layout Subsubsection
Sudden Vision Occlusion
\end_layout

\begin_layout Subsubsection
Delay to First Position Fix
\end_layout

\begin_layout Standard
During localization, it is vital to get position fix as early as possible,
 preferably before motion is commenced.
 However, ORB-SLAM tends to delay position fix ?
\end_layout

\begin_layout Subsection
Implementation and Modification of ORB-SLAM
\end_layout

\begin_layout Subsubsection
Augmentation of Velodyne SLAM Positioning
\end_layout

\begin_layout Standard
Scale problem can be solved if distance of two particular point in ORB-SLAM
 is known based on external reference.
 Thus, in addition to keyframe positions in ORB-SLAM, we also recorded correspon
ding poses in Velodyne SLAM coordinates.
 These poses will be useful to calculate pose correction in Localization-only
 process.
\end_layout

\begin_layout Subsubsection
Map Storage and Restoration
\end_layout

\begin_layout Standard
Map storage consists of three main parts: Keyframes, Map points (and reference
 to which keyframe they belong)
\end_layout

\begin_layout Standard
During map restoration, the system reconstructs the following data structures:
\end_layout

\begin_layout Enumerate
Keyframe list and visibility graph.
 The visibility graph is relation between feature points and map points
 at which they represent, and their visibilities in keyframes.
\end_layout

\begin_layout Enumerate
Map point list
\end_layout

\begin_layout Enumerate
Octree of keyframe position in ORB-SLAM coordinate.
 This tree will be used for fast searching of keyframes during localization
 using augmented positioning.
\end_layout

\begin_layout Subsubsection
Localization-only Using Augmented Positioning
\end_layout

\begin_layout Standard
In the localization-only process, the system depends solely on ORB-SLAM
 method.
 Therefore, external methods such as velodyne LIDAR are not required.
 However, the system modifies position value to correct distance distortion
 according to this formula:
\end_layout

\begin_layout Enumerate
Take original ORB-SLAM positioning result as 
\begin_inset Formula $P_{o}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Search the nearest keyframe from 
\begin_inset Formula $P_{o}$
\end_inset

 using previous octree.
 From here, we take the corresponding Velodyne SLAM position 
\begin_inset Formula $P_{n}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Find the previous keyframe 
\begin_inset Formula $P_{o'}$
\end_inset

 and its corresponding Velodyne SLAM position 
\begin_inset Formula $P_{n'}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Compute distance scale: 
\begin_inset Formula 
\[
s=\frac{\left\Vert P_{n'}-P_{n}\right\Vert }{\left\Vert P_{o'}-P_{o}\right\Vert }
\]

\end_inset


\end_layout

\begin_layout Enumerate
Apply distance scale to correct current position:
\begin_inset Formula 
\[
P_{c}=P_{n}+s(P_{o}-P_{o'})
\]

\end_inset


\end_layout

\begin_layout Standard
In localization-only process, map modification is disabled.
\end_layout

\begin_layout Subsubsection
Place Recognition after Map Restoration
\end_layout

\begin_layout Standard
By default, ORB-SLAM will try to find position against last keyframe when
 it ever get lost.
 However, it is impractical after map restoration, because last keyframe
 is unknown.
 Instead, we modify ORB-SLAM to force it finding the most appropriate keyframe
 using bag-of-word method.
 Keyframe search is also applied when the system lost tracking, to ensure
 that the system always get the keyframe as basis for relocalization.
 The drawback is that keyframe search using bag-of-word is slower than tracking
 using last keyframe.
\end_layout

\begin_layout Section
Evaluation
\end_layout

\begin_layout Standard
We performed four runs of robot by tracing the trajectory mandated by Tsukuba
 Challenge committee.
 In each run, we recorded camera images and performed localization using
 Velodyne LIDAR using two different computers due to high computational
 requirement.
 Clocks of both computers were not synchronized.
 From these runs, we created two maps for localization process.
 Velodyne LIDAR results would be used as ground truth for comparison.
 To reduce computation, camera resolution was reduced to 
\begin_inset Formula $800\times600$
\end_inset

 before processing.
\end_layout

\begin_layout Standard
Two runs were chosen as material for mapping with consideration that those
 runs spanned longest runs without any vision occlusion.
 However, in both run path loop were not executed.
 Therefore, we were not able to evaluate loop closing capability of ORB-SLAM.
 Next, we performed localization test in two runs using both maps.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename map 1 & 3.svg
	lyxscale 30
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Maps (M1) and (M2) of Tsukuba Challenge generated by ORB-SLAM.
 (M1) exhibits heavy distortion towards later parts of trajectory, while
 (M2) has less distortion.
 Both maps are not metrically correct.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ndt-map-3.svg
	lyxscale 30
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Ground truth from Velodyne SLAM.
 In both runs for mapping, real trajectories of robot were about the same.
 This map is metrically correct.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
First Position Fix
\end_layout

\begin_layout Standard
To get initial position fix, ORB-SLAM performs keyframe search based on
 appereances of feature points.
 Search may returns more than one candidates, on which they will be evaluated
 on reprojection error basis.
 The candidates will only be accepted if ...
 In the evaluation run however, the system was slow to get this fix
\end_layout

\begin_layout Subsection
Relocalisation and Tracking
\end_layout

\begin_layout Subsection
Problems
\end_layout

\begin_layout Section
Bibliography
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "ZoteroLibrary"
options "ieeetr"

\end_inset


\end_layout

\end_body
\end_document
