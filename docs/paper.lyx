#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans helvet
\font_typewriter courier
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement h
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 23.5pt
\topmargin 1in
\rightmargin 23.5pt
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Monocular Vision-based Localization using ORB-SLAM with LIDAR-aided Mapping
 in Real-World Robot Challenge
\end_layout

\begin_layout Author
Adi Sujiwo, Tomohito Ando, Eijiro Takeuchi, and Yoshiki Ninomiya
\end_layout

\begin_layout Abstract
In 2015 Tsukuba Challenge, we have realized an implementation of vision-based
 localization based on ORB-SLAM.
 Our method combined mapping based on ORB-SLAM and Velodyne LIDAR SLAM,
 and utilized these maps in localization process using solely monocular
 camera.
 The combined maps delivered better accuracy compared to original ORB-SLAM,
 which suffers from scale ambiguities and experiences map distance distortion.
 This paper reports our experience in using ORB-SLAM for visual localization,
 along with difficulties that we encounter.
\end_layout

\begin_layout Abstract
Keyword: Visual Localization, Autonomous Vehicle, Field Robotics, Tsukuba
 Challenge
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Recently, autonomous driving system and advanced driving assist system are
 actively developed.
 Google already tests autonomous driving in urban environment over 150km.
 Google uses High precision and accurate 3D LiDAR for localization and recogniti
on.
 Such sensors are still expensive relative to other vehicle sensors such
 as camera and millimeter wave range sensor.
 On the other hand, high precision environment digital maps are active research
 areas within autonomous driving systems.
 An example is Mobile Mapping Systems (MMS), which is capable to measure
 accurate 3D point cloud maps by driving on urban road.
 Such 3D maps are basic technology of recent autonomous vehicles.
 
\end_layout

\begin_layout Standard
Real-World Robot Challenge (RWRC) is real-world autonomous navigation challenge
 in City of Tsukuba, Japan.
 The robots need to run over 1km route autonomously.
 One of important techniques to realize autonomous navigation is localization.
 The robots need to keep position accuracy over the course with various
 environment changes such as dynamic obstacles, illumination change, season
 changes and others.
 Most teams in Tsukuba Challenge use sensor fusion approach using LiDAR,
 gyroscope, and odometer.
 Such sensor fusion techniques can compensate weakness of sensor characteristics.
 On the other hand, vision-based approach is not actively used in RWRC.
 In short, objective of this research is to find problems and solutions
 of monocular camera based localization in real-world.
\end_layout

\begin_layout Standard
Despite failure of our team in finishing the track, we have gathered some
 valuable experience in the Tsukuba Challenge.
 Most importantly, we have implemented monocular vision-based localization
 based on ORB-SLAM.
 In the process, we also collected datasets for evaluating vision-based
 localization methods in real world environment.
 
\end_layout

\begin_layout Standard
This paper proposes positioning method using monocular camera with ORB-SLAM
 with LiDAR-aided mapping.
 ORB-SLAM is one of most recent monocular vision-based SLAM method.
 This method estimates position and map from image sequence in real-time.
 Originally, ORB-SLAM is designed to solve SLAM problem.
 However, required performance of localization problem is different from
 SLAM problem; therefore, the method has several problems in adopting localizati
on problems such as robustness and map consistency problem.
 The proposed method has two key points: (1) to estimate metric position
 by using ORB-SLAM with LiDAR-aided mapping, and (2) to solve robustness
 problem using consistent multiple maps.
\end_layout

\begin_layout Standard
Generally, this paper discusses: 1) Benchmark tests of ORB-SLAM in Tsukuba
 challenge environments; 2) Description of ORB-SLAM with LiDAR-aided Mapping
 to solve consistency problems of multiple maps; 3) Experimental results
 and evaluation in Tsukuba-Challenge environments.
 Finally, this paper illustrates capability of vision-based localization
 method with multiple maps to estimate almost similar path with LiDAR base
 localization results over almost course.
\end_layout

\begin_layout Section
Related Works
\end_layout

\begin_layout Subsection
Monocular Vision-Based SLAM
\end_layout

\begin_layout Standard
All monocular SLAM methods are based on 3D reconstruction from multiple
 view of scenes 
\begin_inset CommandInset citation
LatexCommand cite
key "fuentes-pacheco2015visualsimultaneous"

\end_inset

, which in turn is based on Structure from Motion.
 As stated in 
\begin_inset CommandInset citation
LatexCommand cite
key "szeliski1997shapeambiguities"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "lourakis2013accurate"

\end_inset

, all monocular structure from motion methods inherit common scale ambiguities,
 which consists in the fact that the recovered 3D structures and camera
 motion are defined up to an unknown scale factor which cannot be determined
 from image streams alone.
 This is because if the scene and camera are scaled together, this change
 will be indistinguishable in the captured images.
\end_layout

\begin_layout Standard
Besides ORB-SLAM, there have been a number of monocular SLAM with complete
 public implementation.
 Of particular mention are PTAM by 
\begin_inset CommandInset citation
LatexCommand cite
key "klein2007parallel"

\end_inset

, and LSD-SLAM by 
\begin_inset CommandInset citation
LatexCommand cite
key "engel2014lsdslam"

\end_inset

.
 ORB-SLAM itself was described in 
\begin_inset CommandInset citation
LatexCommand cite
key "mur-artal2015orbslam"

\end_inset

.
 Of particular note, the ORB-SLAM uses ORB (Oriented FAST, Rotated BRIEF)
 as main feature detector 
\begin_inset CommandInset citation
LatexCommand cite
key "rublee2011orban"

\end_inset

, and bag-of-words method as place recognition 
\begin_inset CommandInset citation
LatexCommand cite
key "galvez-lopez2012bagsof"

\end_inset

.
 Contrary to previous methods that examine the whole scene and generate
 dense maps, ORB-SLAM generates relatively sparse maps due to use of extracted
 feature points.
 This gives ORB-SLAM potentially smaller maps and faster processing time.
\end_layout

\begin_layout Subsection
LIDAR-based SLAM
\end_layout

\begin_layout Standard
LIDAR-based SLAM is explained in 
\begin_inset CommandInset citation
LatexCommand cite
key "moosmann2011velodyne"

\end_inset

.
 This SLAM method is popular for autonomous vehicle applications, and is
 capable to provide accurate maps and localization.
 Of particular note, this localization method is used in Autoware 
\begin_inset CommandInset citation
LatexCommand cite
key "kato2015anopen"

\end_inset

 as real-world application of autonomous vehicle platform.
\end_layout

\begin_layout Standard
Velodyne-based SLAM takes 3D laser scans as input, and compose the map using
 any popular scan matching algorithms from those scans (for example, normal
 distribution transform 
\begin_inset CommandInset citation
LatexCommand cite
key "takeuchi2006a3d"

\end_inset

).
\end_layout

\begin_layout Subsection
Tsukuba Challenge
\end_layout

\begin_layout Standard
In most of the course of Tsukuba Challenge, almost all teams use LIDAR-based
 localization method 
\begin_inset CommandInset citation
LatexCommand cite
key "morales2009autonomous"

\end_inset

.
 These methods use sensor fusion approach with LIDAR and odometer.
 One of the important technique is to use good dead reckoning method such
 as calibrated gyroscope.
 
\end_layout

\begin_layout Standard
There had been some trials of monocular vision-based localization in RWRC.
 One of the those efforts is 
\begin_inset CommandInset citation
LatexCommand cite
key "ohshima2008teachingplayback"

\end_inset

; this method is basically topological SLAM, by following image sequence
 and estimate pose using feature points.
 Comparing with the above methods, our method tries to get metrically correct
 positioning, similar to LIDAR-based navigation but using monocular camera.
 This method is familiar to sensor fusion with other metric localization
 method such as LiDAR, GNSS, and dead reckoning methods.
\end_layout

\begin_layout Section
Monocular Vision-Based Localization
\end_layout

\begin_layout Standard
In order to provide multiple time localization, our implementation of ORB-SLAM
 consists of two parts: mapping and localization-only.
 The mapping process runs similar with original implementation, with addition
 of map storage in the end of mapping run.
 Meanwhile, the localization-only process starts with map restoration from
 data saved previously by mapping process.
 Next, localization-only proceeds similar with mapping.
 However, map modification is disabled in relocalization process.
 
\end_layout

\begin_layout Subsection
Description of ORB-SLAM
\end_layout

\begin_layout Standard
The ORB-SLAM main process creates an environmental map which consists of
 keyframes and map points.
 Each keyframe stores its position in ORB-SLAM coordinate and a list of
 2D feature points.
 The whole ORB-SLAM process consists of three parallel threads: tracking,
 local mapping and loop closing.
 Relations between these processes are illustrated in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Flowchart-of-ORB-SLAM"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Feature Detection
\end_layout

\begin_layout Standard
The first step in all 3D reconstruction is to find feature points in each
 frame.
 ORB-SLAM uses ORB (Oriented FAST, Rotated BRIEF) described in 
\begin_inset CommandInset citation
LatexCommand cite
key "rublee2011orban"

\end_inset

.
 This detector has advantages such as faster computation and lower storage
 requirement (each descriptor only needs 32 bytes), in addition to resistance
 against rotation and noise.
 
\end_layout

\begin_layout Subsubsection
Map Initialization
\end_layout

\begin_layout Standard
The goal of the map initialization is to compute the relative pose between
 two frames to triangulate an initial set of map points
\begin_inset CommandInset citation
LatexCommand cite
key "mur-artal2015orbslam"

\end_inset

, which then will be used for keyframe tracking.
 ORB-SLAM uses combination of homography and fundamental matrices inside
 RANSAC scheme in order to build motion and structure recovery as described
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "faugeras1988motionand"

\end_inset

.
 When this stage is succesful, the system will have an initial set of keyframes
 and map points for which tracking may proceed.
 However, tracking may fail shortly after initial map is built; when this
 happens the initial map is reset and started over.
\end_layout

\begin_layout Subsubsection
Tracking and Local Mapping
\end_layout

\begin_layout Standard
The tracking thread is responsible for providing localization and map building.
 After ORB corners are detected, the tracking thread develops map incrementally
 over recovered 3D map points, meanwhile computing camera poses.
 To speed up the process, tracking works in a smaller subset of the whole
 map, called local map, that covers current visible and a number of connected
 keyframes.
 Tracking thread also performs map 
\begin_inset Quotes eld
\end_inset

clean-up
\begin_inset Quotes erd
\end_inset

, which is an act of culling bad map points and keyframes.
 
\end_layout

\begin_layout Standard
To perform tracking, there are three modes that can be used.
 First is relocalization by searching all keyframes by bag of words; this
 is the slowest but indispensable when recovering from lost tracking.
 Second is tracking the local map as described above.
 Alternatively, third choice is tracking using constant velocity model.
 The third choice is fastest and may be most often used mode.
 However, it may be inaccurate.
\end_layout

\begin_layout Standard
Occasionally, tracking thread will perform local bundle adjustment (BA)
 to optimize current local map.
 This action moves the keyframes in local map, and potentially marks some
 keyframes as outliers for removal.
\end_layout

\begin_layout Subsubsection
Loop Closing
\end_layout

\begin_layout Standard
Loop-closure detection is crucial for enhancing the accuracy of SLAM algorithms,
 both topological and metrical.
 This problem consists of detecting when the robot has returned to a past
 location after having discovered new terrain for a while.
 Such detection makes it possible to increase the precision of the actual
 pose estimation.
\end_layout

\begin_layout Standard
Essentially, loop closing in ORB-SLAM uses image-to-map approach 
\begin_inset CommandInset citation
LatexCommand cite
key "williams2009acomparison"

\end_inset

 for loop closing.
 First, it takes last processed keyframe and search the loop candidate keyframes
 in local map using bag of words 
\begin_inset CommandInset citation
LatexCommand cite
key "galvez-lopez2012bagsof"

\end_inset

.
 Next, similarity transformation is computed.
 Loop correction is performed by inserting new edges in covisibility graph
 and fixing connectivity between loop candidate and surrounding keyframes.
 Next, ORB-SLAM performs pose graph optimization that distributes loop closing
 errors by moving the candidate and its connected keyframes.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename orb-slam overview.eps
	width 95col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
ORB-SLAM System Overview
\begin_inset CommandInset label
LatexCommand label
name "fig:Flowchart-of-ORB-SLAM"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Problems in ORB-SLAM
\end_layout

\begin_layout Subsubsection
Scale Problem
\end_layout

\begin_layout Standard
ORB-SLAM emits localization results based on maps in its own coordinate
 system (and not metrically correct), that are not free from distortion
 due to scale ambiguities.
 This problem is inherent in almost all vision-based localization methods
 or even all 3D reconstruction method 
\begin_inset CommandInset citation
LatexCommand cite
key "hartley2003multiple"

\end_inset

.
 In some occasion, result maps may suffer heavy deformation, as illustrated
 in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ORB-SLAM-robot-trajectory"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename scale problem.eps
	lyxscale 70
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
ORB-SLAM map trajectories from different time are plotted in (1) and (2).
 (3) is ground truth.
 (4) is zoomed part of red rectangle in (1).
\begin_inset CommandInset label
LatexCommand label
name "fig:ORB-SLAM-robot-trajectory"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:ORB-SLAM-robot-trajectory"

\end_inset

, it is clear that ORB-SLAM generates very deformed trajectory shapes compared
 to ground truth (trajectory generated by Velodyne SLAM at corresponding
 time).
 By zooming-in parts in red square, it is revealed that most later keyframes
 clump in a small area of the ORB-SLAM map.
 Applying point distances acquired from ground truth, we get a trajectory
 that is closer to ground truth (5) but still has wrong shape.
 
\end_layout

\begin_layout Subsubsection
Lack of Support for Lifelong Mapping
\end_layout

\begin_layout Standard
Original design for ORB-SLAM is single run for both localization and mapping,
 and there is not any features to store in-memory map.
 However, it is desirable that mapping can be done multiple times with same
 path, so map saving and restoration is essential.
 This feature is also useful for improving map robustness in changing condition
 
\begin_inset CommandInset citation
LatexCommand cite
key "konolige2009towards"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Sudden Vision Occlusion
\end_layout

\begin_layout Standard
Any disturbances in camera vision that make it unable to view previously
 tracked feature points may cause ORB-SLAM to lost tracking.
 These include vision occlusion on behalf of camera and sudden rotation
 of the robot.
 The default behaviour of ORB-SLAM to reacquire tracking after lost event
 is by performing relocalization from last keyframe (i.e.
 guess pose from last known position).
 However, this prevent the system to reacquire position especially when
 the robot never reverses motion, and contrary to described behaviour in
 original paper of ORB-SLAM.
 Our solution is to force relocalization by searching all keyframes in database.
\end_layout

\begin_layout Section
ORB-SLAM with LIDAR-aided Mapping
\end_layout

\begin_layout Standard
Main output of ORB-SLAM mapping is a set of keyframes.
 As explained above, our main goal is to take this map and compute localization
 during robot runs as guidance for navigation system, be it a robot or an
 autonomous car.
 Therefore it is necessary to get localization results that are metrically
 acurate.
 However as illustrated in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ORB-SLAM-robot-trajectory"

\end_inset

, it would be very difficult to get accurate position using deformed maps
 from ORB-SLAM.
\end_layout

\begin_layout Standard
To formalize the map and localization process, we state the following notation.
 An ORB-SLAM map is a set of pose
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
There are other important data, but not our concern yet.
\end_layout

\end_inset

: 
\begin_inset Formula $M=\left\{ P_{i}|0\leq i<N-1\right\} $
\end_inset

, where 
\begin_inset Formula $N$
\end_inset

 is the number of keyframes until mapping process is stopped.
 Each pose 
\begin_inset Formula $P$
\end_inset

 consists of translation and rotation (in quaternion) in 3D, so 
\begin_inset Formula $P$
\end_inset

 is a vector of seven elements: 
\begin_inset Formula $P=(\mathbf{t},\mathbf{q})=(x,y,z,q_{x},q_{y},q_{z},q_{w})$
\end_inset

.
\end_layout

\begin_layout Subsection
Metric Transformation from ORB-SLAM
\end_layout

\begin_layout Standard
Scale problem can be solved if distance of two particular points in ORB-SLAM
 coordinate is known based on external reference.
 In longer run, this association must be done correctly so that error accumulati
on of scale correction is eliminated.
 Therefore, the external reference must have high accuracy.
 In our case, we chose Velodyne-based SLAM as reference due to immediate
 availability.
 Other positioning methods may also be used, such as GPS or odometry as
 long as error corrections are provided 
\begin_inset CommandInset citation
LatexCommand cite
key "burschka2004vgpsslam"

\end_inset

.
\end_layout

\begin_layout Standard
In the localization process, the system depends solely on ORB-SLAM method.
 Therefore, external methods such as LIDAR SLAM are not required.
 However, the system modifies position value to correct distance deformation
 according to this formula.
 This transformation is illustrated in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:metric-convertion"

\end_inset


\end_layout

\begin_layout Enumerate
Take original ORB-SLAM position from localization as 
\begin_inset Formula $P_{o}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Search the nearest keyframe in map from 
\begin_inset Formula $P_{o}$
\end_inset

 as 
\begin_inset Formula $P_{k}$
\end_inset

.
 From here, we take the corresponding Velodyne SLAM position 
\begin_inset Formula $P_{n}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Find the previous offset keyframe 
\begin_inset Formula $P_{k'}$
\end_inset

 and its corresponding Velodyne SLAM position 
\begin_inset Formula $P_{n'}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Compute scale correction factor: 
\begin_inset Formula 
\[
s=\frac{\left\Vert \mathbf{t}_{n'}-\mathbf{t}_{n}\right\Vert }{\left\Vert \mathbf{t}_{k'}-\mathbf{t}_{k}\right\Vert }
\]

\end_inset


\end_layout

\begin_layout Enumerate
Apply distance scale to correct current position as 
\begin_inset Formula $P_{c}$
\end_inset

 :
\begin_inset Formula 
\[
P_{r}=P_{o'}^{-1}P_{o}
\]

\end_inset


\begin_inset Formula 
\[
P_{r'}=(s\mathbf{t}_{r},\mathbf{q}_{r})
\]

\end_inset


\begin_inset Formula 
\[
P_{c}=P_{n}P_{r'}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename metric_convertion.eps
	lyxscale 60
	width 60col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Illustration of metric transformation
\begin_inset CommandInset label
LatexCommand label
name "fig:metric-convertion"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
External Localization Reference Using NDT Scan Matching
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 3d view of orb map.png
	lyxscale 30
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
3D view of Tsukuba Challenge map generated by NDT Scan Matching from Velodyne
 Scans
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Map Storage and Restoration
\end_layout

\begin_layout Standard
Map storage consists of three main parts: keyframes, map points, and keyframe
 relationships.
 Each keyframe stores camera pose 
\begin_inset Formula $P_{o}$
\end_inset

 in ORB-SLAM coordinate, camera intrinsic parameters, all ORB feature points
 recorded at keyframe creation, and LIDAR pose 
\begin_inset Formula $P_{n}$
\end_inset

 in NDT coordinate recorded at keyframe creation.
 
\end_layout

\begin_layout Standard
During map restoration, the system reconstructs the following data structures:
\end_layout

\begin_layout Enumerate
List of keyframes and their relationship.
\end_layout

\begin_layout Enumerate
Map point list
\end_layout

\begin_layout Enumerate
Octree of keyframe position in ORB-SLAM coordinate.
 This tree will be used for fast searching of keyframes during localization
 using augmented positioning.
\end_layout

\begin_layout Standard
By default, ORB-SLAM will try to find position against last keyframe when
 it ever get lost.
 However, it is impractical after map restoration, because last keyframe
 is unknown.
 Instead, we modify ORB-SLAM to force it finding the most appropriate keyframe
 using bag-of-word method.
 Keyframe search is also applied when the system lost tracking, to ensure
 that the system always get the keyframe as basis for relocalization.
 The drawback is that keyframe search using bag-of-word is slower than tracking
 using last keyframe.
\end_layout

\begin_layout Subsection
Using Multiple Maps
\end_layout

\begin_layout Standard
During experiments, we found that maps from same place but different time
 will deliver different results.
 Therefore it is logical to combine results from two maps in order to: 1)
 alternately provide localization whenever one of the maps fails; 2) reduce
 errors from all maps.
 In this regard, any method for sensor fusion may be used.
\end_layout

\begin_layout Section
Evaluation in Tsukuba Challenge Environment
\end_layout

\begin_layout Standard
We performed four runs of robot by traversing the trajectory mandated by
 Tsukuba Challenge committee.
 In each run, we recorded camera images and performed localization using
 Velodyne LIDAR using two different computers due to high computational
 requirement.
 Clocks of both computers were not synchronized.
 From these runs, we created two maps for localization process.
 Velodyne LIDAR results would be used as ground truth for comparison.
 To reduce computation, camera resolution was reduced to 
\begin_inset Formula $800\times600$
\end_inset

 before processing.
\end_layout

\begin_layout Standard
Two runs were chosen as material for mapping with consideration that those
 runs spanned longest runs without any vision occlusion.
 However, in both run path loop were not executed.
 Therefore, we were not able to evaluate loop closing capability of ORB-SLAM.
 First mapping run was conducted at 6th November 13:44, at which weather
 condition was clear with few obstacles in the path.
 Second mapping was at 7th November 11:30, during which weather condition
 was cloudy and few people along the path.
 For testing runs, we chose first run at 3rd November 15:00; this day was
 a holiday with many people along the track, the weather was very clear
 with strong lighting contrast between sky and ground.
 Second testing run was from 7th November 14:20; there were only a few pedestria
ns and many other robot teams but very soft contrast.
 For all mapping and running runs, there were a few occasion of heavy flares
 when the camera faced to the sun, while the tracks were covered by falling
 leaves.
\end_layout

\begin_layout Standard
The map trajectory that we use is drawn in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ORB-SLAM-robot-trajectory"

\end_inset

 (1) and (2).
 Note that those maps are heavily deformed compared to ground truth in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Ground-truth"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ndt-map-3.eps
	lyxscale 70
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Ground truth from Velodyne SLAM.
 In both runs for mapping, real trajectories of robot were about the same.
 This map is metrically correct.
\begin_inset CommandInset label
LatexCommand label
name "fig:Ground-truth"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Experimental Settings
\end_layout

\begin_layout Standard
Our robot is derived from Segway RMP-200 platform, using Grasshopper3 camera
 and Velodyne HDL-32 LIDAR.
 The robot ran on the mandated Tsukuba Challenge with speed under 1 m/s.
 In the course of run, robot may often encounter dynamic obstacles such
 as human or bycicle, which necessitated action by operator to either stop
 or maneuver the robot motion.
 Our robot setup is shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Robot-Setup"

\end_inset

.
\end_layout

\begin_layout Standard
Our track in Tsukuba Challenge was vastly different from original ORB-SLAM
 paper evaluation, which used New College dataset.
 The figures below show typical scenarios which may be found during Tsukuba
 Challenge.
 In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Starting-Point"

\end_inset

, the starting point featured paveblock tracks, trees and building in far
 background.
 Most areas are pedestrian tracks with thick cover from trees and falling
 leaves in the ground (figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Pedestrian"

\end_inset

).
 There were also many areas with strong contrast, as in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Strong-contrast"

\end_inset

.
 This situation may confuse automatic exposure system of cameras, and makes
 it difficult to detect feature points.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename robot view.eps
	lyxscale 30
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Robot for evaluation
\begin_inset CommandInset label
LatexCommand label
name "fig:Robot-Setup"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename starting point.jpg
	lyxscale 50
	width 95col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Starting Point
\begin_inset CommandInset label
LatexCommand label
name "fig:Starting-Point"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pedestrians.eps
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Typical situation in Tsukuba Challenge: pedestrian tracks covered in falling
 leaves
\begin_inset CommandInset label
LatexCommand label
name "fig:Pedestrian"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename strong contrast.jpg
	lyxscale 50
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Lighting situation featured many areas with strong contrast due to shadow
 cast
\begin_inset CommandInset label
LatexCommand label
name "fig:Strong-contrast"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Map Saving and Restoration
\end_layout

\begin_layout Standard
In our experience, map saving and restoration does not affect ORB-SLAM performan
ce.
 In fact, the system gains useful capability: map building can now be done
 incrementally, using same location in different times.
 This is useful for example, to build lifelong map in different situation
 such as weather and day/night.
 An example of map building is illustrated in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Partial-map"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename partial map.png
	lyxscale 70
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Partial map being built after loading
\begin_inset CommandInset label
LatexCommand label
name "fig:Partial-map"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
First Position Fix
\end_layout

\begin_layout Standard
To get initial position fix, ORB-SLAM performs keyframe search based on
 appearances of feature points.
 Search may returns more than one candidates, on which they will be evaluated
 on reprojection error basis.
 Only one candidate is accepted, and it must have at least 15 map points
 that match with feature points in current frame.
 In the evaluation run however, the system was slow to get the fix due to
 insufficient matches.
 One possible correction to fasten initial fix is by increasing number of
 feature points from ORB computation.
 However, this approach heavily slows down the search process, and does
 not always correlate to faster fix.
\end_layout

\begin_layout Standard
Map initialization is also another problem.
 During experiment, we found that initialization will succeed (and not getting
 false initialization) whenever robot is moved, either rotation or translation.
 In our experiences, false map initialization and slow position fix can
 be solved by increasing number of extracted ORB points (by default this
 is 1000) to 2500.
 The drawback is higher computation times per frame.
 However, other benefits from increasing this number are better resistance
 against lens flares and partial occlusion.
\end_layout

\begin_layout Subsection
Tracking and Relocalization
\end_layout

\begin_layout Standard
During our experiment, we found that ORB-SLAM is resistant against occasional
 and partial vision occlusion.
 Partial occlusion includes lens flares and human covering background images.
 Total occlusion however, may cause the system to miss tracking and difficult
 to recover.
 This lost tracking explains existence of blank areas below (part of trajectory
 that has no bold parts).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename tracking 1.png
	lyxscale 60
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
ORB-SLAM performed tracking
\begin_inset CommandInset label
LatexCommand label
name "fig:tracking"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Common situation and tracking of ORB-SLAM is depicted in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:tracking"

\end_inset

.
 The figure illustrates a frame, taken in Oshimizu park area with background
 of far buildings and some trees in foreground.
 There were also some people in the scene.
 Most of the ORB points (and map points, shown in green dots) fell in the
 trees and ground, but very few of those that were in background.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:traversing-the-map"

\end_inset

 depicts visualization of the robot when it traveled along previously created
 map.
 The map was created from different day.
 Note the slightly out-of-track position of the robot.
 In this situation, tracking was maintained.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename robot traveled along the map.png
	lyxscale 60
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Robot traversed previously created map
\begin_inset CommandInset label
LatexCommand label
name "fig:traversing-the-map"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:lost-tracking"

\end_inset

 depicts a situation when the robot performed violent rotation and about
 to lost tracking.
 Note the absence of ORB feature points in the right portion of image frame.
 At right, the axis shows that robot was in right track, but robot is oriented
 to a place without map points.
 The blue axis represents front.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename about to lose tracking.png
	lyxscale 60
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
ORB-SLAM about to lost tracking
\begin_inset CommandInset label
LatexCommand label
name "fig:lost-tracking"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Using-Map-(M1)"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Using-Map-(M2)"

\end_inset

 show the results from using map 1 and 2.
 Next figure (
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:joint-map-result"

\end_inset

) shows result using two maps.
 The differences of using both maps are: 1) map 1 is quicker in obtaining
 initial fix but has larger 
\begin_inset Quotes eld
\end_inset

blank spot
\begin_inset Quotes erd
\end_inset

 due to lost tracking; 2) map 2 delivers more coverage due to less lost
 tracking.
 In all maps, initial position fix were obtained at substantial time after
 motion were commenced.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename orb-1.svg
	lyxscale 40
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Using Map (M1)
\begin_inset CommandInset label
LatexCommand label
name "fig:Using-Map-(M1)"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename orb-2.svg
	lyxscale 40
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Using Map (M2)
\begin_inset CommandInset label
LatexCommand label
name "fig:Using-Map-(M2)"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename jointmap_result.svg
	lyxscale 90
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Localization Using Combined Maps (M1) and (M2)
\begin_inset CommandInset label
LatexCommand label
name "fig:joint-map-result"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Loop Closing
\end_layout

\begin_layout Standard
The Tsukuba Challenge track features some areas that are ideal to test loop
 closing subsystem of any SLAM system.
 However, ORB-SLAM was not able to perform loop closing when they were supposed
 to take place.
 One important area is the Oshimizu park subtrack, that features a small
 roundabout with far buildings as background.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename loop closing failure.png
	lyxscale 70
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
A particular place where loop closing failed to take place.
 Image contrast has been adjusted to lighten dark areas.
 Notice that most ORB points (green) fell in the background cloud.
\begin_inset CommandInset label
LatexCommand label
name "fig:loop-closing-failure"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Accuracy of Corrected Localization
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename path inspection.svg
	lyxscale 70
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Example of visual comparison of Modified ORB-SLAM trajectory against ground
 truth (blue curve).
 Map 1 result is depicted in green, while map 2 in red.
\begin_inset CommandInset label
LatexCommand label
name "fig:turning-map"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:turning-map"

\end_inset

 depicts a turning situation continued by straight path.
 In the turning, modified ORB-SLAM shows deviations compared to ground truth,
 while straight path has less deviation.
 It is also clear that each maps give different results, despite running
 in the exact same path and time.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename errors1.eps
	lyxscale 50
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename errors2.eps
	lyxscale 50
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename errorsj.eps
	lyxscale 50
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Error graphs from all maps used in the experiment
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="right" valignment="top" width="0">
<column alignment="right" valignment="top" width="0">
<column alignment="right" valignment="top" width="0">
<row>
<cell multirow="3" alignment="left" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Map
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Errors (in m)
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Average
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Std.
 Dev
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Maximum
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Map 1
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.08
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.21
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Map 2
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.06
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.09
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.67
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Joint Map
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.07
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.61
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Errors from modified ORB-SLAM compared to ground truth
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusions and Future Work
\end_layout

\begin_layout Standard
This work reported our approach for localization solutions in Tsukuba Challenge.
 Within limitations of our system, the experiments confirmed that vision-based
 localization using augmented maps from vision and LIDAR-based methods are
 capable to provide localization that are precise enough.
\end_layout

\begin_layout Standard
Contrary to original results, ORB-SLAM was unable to give excellent results
 in dynamic environment such as Tsukuba Challenge.
 We suspect that due to significant time between mapping and localization,
 ORB-SLAM was unable to perform place recognition using its bag-of-words
 method.
 This is supported from observation that most ORB feature points fell in
 the ground covered in falling leaves.
\end_layout

\begin_layout Standard
Despite a good results, there are still some room for improvements that
 we can propose.
 Primarily, there must be an effort to cover the whole spectrum of localization,
 critically when the vision-based localization fails.
 In this regard, we suggest next works to integrate other low-cost methods
 such as odometry and gyroscope solutions.
 In the mapping, we also suggest replacing the LIDAR to other global SLAM
 methods, such as GPS .
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "ZoteroLibrary"
options "ieeetr"

\end_inset


\end_layout

\end_body
\end_document
